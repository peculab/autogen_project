import os
import asyncio
import json
import threading
import pandas as pd
from dotenv import load_dotenv, find_dotenv
from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
from werkzeug.utils import secure_filename
from google import genai
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.messages import TextMessage
from EMOwithSnow import generate_mood_trend_plot

# âœ… åˆå§‹åŒ– Flask èˆ‡ SocketIO
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
socketio = SocketIO(app, async_mode='threading')
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# âœ… è¼‰å…¥ .env ä¸¦åˆå§‹åŒ– Gemini
dotenv_path = find_dotenv()
#print(f"âœ… ç›®å‰ä½¿ç”¨çš„ .env è·¯å¾‘: {dotenv_path}")
load_dotenv(dotenv_path)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
#print(GEMINI_API_KEY)

# âœ… å»ºç«‹ Gemini å®¢æˆ¶ç«¯èˆ‡æ¨¡å‹
client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))

# âœ… å°è£ autogen client
class GeminiChatCompletionClient:
    def __init__(self, model="gemini-1.5-flash-8b"):
        self.model = model
        self.model_info = {"vision": False}

    async def create(self, messages, **kwargs):
        parts = []
        for m in messages:
            if hasattr(m, 'content'):
                parts.append(str(m.content))
            elif isinstance(m, dict) and 'content' in m:
                parts.append(str(m['content']))
        content = "\n".join(parts)
        response = client.models.generate_content(
            model=self.model,
            contents=content
        )
        return type("Response", (), {
            "text": response.text,
            "content": response.text,
            "usage": {
                "prompt_tokens": {"value": 0},
                "completion_tokens": {"value": 0}
            }
        })

model_client = GeminiChatCompletionClient()

# âœ… å¤š Agent åˆ†æï¼ˆä¿ç•™åŸä¾†ç¨‹å¼ç¢¼ï¼‰
from multiagent import run_multiagent_analysis

# âœ… Flask è·¯ç”±
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return 'No file part', 400
    file = request.files['file']
    if file.filename == '':
        return 'No selected file', 400
    if file:
        filename = secure_filename(file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(file_path)
        socketio.emit('update', {'message': 'ğŸŸ¢ æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼Œé–‹å§‹åˆ†æä¸­...'})
        threading.Thread(target=background_task, args=(file_path,)).start()
        return 'File uploaded and processing started.', 200

def background_task(file_path):
    try:
        df = pd.read_csv(file_path)
        user_id = os.path.splitext(os.path.basename(file_path))[0]
        plot_path = generate_mood_trend_plot(user_id, df)
        socketio.emit('plot_generated', {'plot_url': '/' + plot_path})
        asyncio.run(run_multiagent_analysis(socketio, user_id, df))
    except Exception as e:
        socketio.emit('update', {'message': f"âŒ åˆ†æéç¨‹å‡ºç¾éŒ¯èª¤: {str(e)}"})

# âœ… Gemini èŠå¤©å€æ”¯æ´å³æ™‚å›æ‡‰
@socketio.on('chat_message')
def handle_user_chat(data):
    user_message = data.get('message', '').strip()
    if not user_message:
        return
    
    socketio.emit('ai_reply', {'message': 'ğŸ’¬ Gemini æ­£åœ¨æ€è€ƒä¸­ï¼Œè«‹ç¨å€™...'})

    def chat_reply():
        try:
            response = client.models.generate_content(
                model="gemini-2.5-pro-exp-03-25",
                contents=f"ä½ æ˜¯å¿ƒç†è«®å•†å¸«ï¼Œè¦ç”¨æ­£é«”ä¸­æ–‡ä¾†å›æ‡‰ï¼Œä»¥ä¸‹çš„å°è©±å…§å®¹ï¼š{user_message}",
            )
            
            reply = response.text.strip()
            #print("ğŸ” Gemini å³æ™‚å›æ‡‰ï¼š", reply)
            socketio.emit('ai_reply', {'message': reply})
        except Exception as e:
            #print("âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š", str(e))
            socketio.emit('ai_reply', {'message': f"âš ï¸ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"})

    threading.Thread(target=chat_reply).start()

if __name__ == '__main__':
    socketio.run(app, debug=True)
